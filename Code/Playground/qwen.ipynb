{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d78b03",
   "metadata": {},
   "source": [
    "# Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c63a48f-63cb-45e1-93f0-6e5dbf2d2811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  3 10:32:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 66%   63C    P0            282W /  450W |   23807MiB /  24564MiB |     98%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e8f21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import textworld\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966b18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "torch.set_default_device('cuda')\n",
    "torch.cuda.device(\"cuda\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7220f-2c8f-4033-912a-96f973c608e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-1.7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26066610-3c2e-42e0-863d-7af06e42db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-4B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e844c698-9d7f-4af9-83bc-79f002d8966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-4B-FP8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a89bf-fdc4-4a35-b4d0-22b230ecdc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1372c1a2213644afb67aae5f6c8d9f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d047ced8b549fdbe5e5aa8dd19ddd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7910578498a244daab62d851da9ec1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445d8309f30e4f029aa73ccdea0b0092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80aeb807d66b49848caa9a634dca4b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/894 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdc09e73f784a34ba4980b14400d961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcbd4cb4c444652a66b64a81a17cc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb9d71e26524374b4b4127a39121162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fbd6f481614d8b96d91b70b48d2151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/778M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c28c933d1e94b8386cf170918a40080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455cc0adaf1d4a669a29ebb28876f719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca1209d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07285302-a27f-4f24-9dff-91e0ba5e7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "think_command_id = 151668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f206ab06-c9f9-4eb0-9ff1-547dcc7c725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(inference took 1.840 seconds)\n",
      "thinking content: <think>\n",
      "\n",
      "</think>\n",
      "content: The optimal strategy for winning Space Invaders is to carefully manage your ship's position, use the laser to eliminate enemies efficiently, and conserve ammo by targeting weak points and avoiding unnecessary shots.\n"
     ]
    }
   ],
   "source": [
    "# prepare the model input\n",
    "prompt = \"Explain what the optimal strategy for winning Space Invaders is in one sentence. /no_think\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "start = time.time()\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32000\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"(inference took {(end - start):.3f} seconds)\")\n",
    "\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # index finding </think>\n",
    "    index = len(output_ids) - output_ids[::-1].index(think_command_id)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33550f55-c5df-4b15-9191-89d8778732b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Explain what the optimal strategy for winning Space Invaders is in one sentence.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The optimal strategy for winning Space Invaders is to shoot the invaders when they are in the top row, avoid being hit by their fire, and shoot them when they are in the middle of their formation to maximize damage while minimizing risk.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc161107-00b1-4997-bc60-007808c1fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f55d82",
   "metadata": {},
   "source": [
    "## Context size and shifting window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cb13705",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_system = \"<|im_start|>system\\n\"\n",
    "token_endofturn = \"<|im_end|>\"\n",
    "token_user = \"<|im_start|>user\\n\"\n",
    "token_assistant = \"<|im_start|>assistant\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a776f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed: 123\n",
      "Game generated: /Main/LLM-PTG/Code/Playground/tw_games/game.ulx\n"
     ]
    }
   ],
   "source": [
    "!tw-make custom --world-size 2 --quest-length 3 --nb-objects 10 --output tw_games/game.ulx -f -v --seed 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1d0ab23-528f-41a9-aa8a-299812307a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tw-v15\n"
     ]
    }
   ],
   "source": [
    "import textworld.gym\n",
    "env_id = textworld.gym.register_game('tw_games/game.ulx')\n",
    "print(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed9baab3-4354-4bf5-b4ba-4ca79ddc82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = textworld.gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af509bcc-64f6-4b56-93ff-02ebc38149a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an assistant playing a textual game.\n",
    "You analyze the information given carefully and reply in the form \\\"verb noun\\\", e.g. \\\"open box\\\" or \\\"take key\\\".\n",
    "/no_think\n",
    "\"\"\"\n",
    "context = token_system + system_prompt + token_endofturn\n",
    "\n",
    "try:\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        game_status = env.render(mode=\"text\")\n",
    "        print(game_status)\n",
    "        context += token_user + game_status + token_endofturn + token_assistant\n",
    "        \n",
    "        start = time.time()    \n",
    "        input_ids = tokenizer.encode(\n",
    "            context,\n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        output = model.generate(\n",
    "            input_ids.to(\"cuda\"),\n",
    "            max_new_tokens=100,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(output[0, input_ids.shape[1]:], skip_special_tokens=True).replace(\"<think>\", \"\").replace(\"</think>\", \"\").strip(\"\\n\")\n",
    "        context += response + token_endofturn\n",
    "        print(response)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(f\"Inference took {(end - start):.3f} seconds\")\n",
    "        \n",
    "        command = response if len(response.split()) <= 5 else \"look around\"\n",
    "        game_state, score, done, infos = env.step(command)\n",
    "\n",
    "    env.render()  # Final message.\n",
    "except KeyboardInterrupt:\n",
    "    pass  # Quit the game.\n",
    "\n",
    "print(\"Played {} steps, scoring {} points.\".format(game_state.moves, game_state.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a68d4-15a9-444c-b66c-a646f0ea571d",
   "metadata": {},
   "source": [
    "# Test with cumulative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "190ead67-f292-4d65-9d6a-93bf3cc06159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textworld\n",
    "\n",
    "# create a game\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 18 --test --silent -f --output games/test-game.z8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d4868b5-ffea-41e2-a50e-45034af6980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a play function for playing + recording scores\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import textworld.gym\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def play(agent, path, max_steps=100, nb_episodes=10, verbose=True):\n",
    "    torch.manual_seed(46)  # For reproducibility when using action sampling.\n",
    "\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
    "\n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
    "\n",
    "    env_id = textworld.gym.register_games(gamefiles,\n",
    "                                          request_infos=infos_to_request,\n",
    "                                          max_episode_steps=max_steps)\n",
    "    env = textworld.gym.make(env_id)  # Create a Gym environment to play the text game.\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path), end=\"\")\n",
    "        else:\n",
    "            print(os.path.basename(path), end=\"\")\n",
    "\n",
    "    # Collect some statistics: nb_steps, final reward.\n",
    "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
    "    for no_episode in range(nb_episodes):\n",
    "        obs, infos = env.reset()  # Start new episode.\n",
    "\n",
    "        score = 0\n",
    "        done = False\n",
    "        nb_moves = 0\n",
    "        while not done:\n",
    "            command = agent.act(obs, score, done, infos)\n",
    "            obs, score, done, infos = env.step(command)\n",
    "            nb_moves += 1\n",
    "\n",
    "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
    "\n",
    "        if verbose:\n",
    "            print(\".\", end=\"\")\n",
    "        avg_moves.append(nb_moves)\n",
    "        avg_scores.append(score)\n",
    "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
    "\n",
    "    env.close()\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
    "        else:\n",
    "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c6f3dca-1b16-4dba-afb9-e97c20319b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agents\n",
    "\n",
    "from typing import Mapping, Any\n",
    "import numpy as np\n",
    "import textworld.gym\n",
    "\n",
    "class RandomAgent(textworld.gym.Agent):\n",
    "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
    "    def __init__(self, seed=1234):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands=True)\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        return self.rng.choice(infos[\"admissible_commands\"])\n",
    "\n",
    "class HFAgent(textworld.gym.Agent):\n",
    "    \"\"\"LLM from HuggingFace that acts as an agent.\"\"\"\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    context = \"\"\n",
    "    token_system = \"<|im_start|>system\\n\"\n",
    "    token_endofturn = \"<|im_end|>\\n\"\n",
    "    token_user = \"<|im_start|>user\\n\"\n",
    "    token_assistant = \"<|im_start|>assistant\\n\"\n",
    "    system_prompt = \"\"\"\n",
    "You are an assistant playing a textual game.\n",
    "The user gives you information on the environment and you reply exclusively in the form \\\"verb noun\\\", like \\\"open box\\\" or \\\"take key\\\".\n",
    "/no_think\n",
    "\"\"\"\n",
    "    first_move = False\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.initialize_context()\n",
    "\n",
    "    def initialize_context(self):\n",
    "        self.context = self.token_system + self.system_prompt + self.token_endofturn\n",
    "        self.first_move = True\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands=True)\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "\n",
    "        if done:\n",
    "            self.initialize_context() # resets context\n",
    "        if self.first_move:\n",
    "            self.first_move = False\n",
    "            return \"help\"\n",
    "        \n",
    "        try:\n",
    "            self.context += self.token_user + obs + self.token_endofturn\n",
    "            self.context += self.token_assistant # induces model to generate answer\n",
    "            \n",
    "            input_ids = self.tokenizer.encode(\n",
    "                self.context,\n",
    "                return_tensors = \"pt\")\n",
    "            \n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids.to(\"cuda\"),\n",
    "                max_new_tokens = 100,\n",
    "                eos_token_id = self.tokenizer.eos_token_id\n",
    "                )\n",
    "            output_ids = generated_ids[0][len(input_ids[0]):].tolist() \n",
    "            \n",
    "            # parsing thinking content\n",
    "            try:\n",
    "                # index finding </think>\n",
    "                index = len(output_ids) - output_ids[::-1].index(think_command_id)\n",
    "            except ValueError:\n",
    "                index = 0\n",
    "            response = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "            \n",
    "            self.context += response + self.token_endofturn\n",
    "\n",
    "            if len(response.split()) <= 6:\n",
    "                command = response\n",
    "            else:\n",
    "                command = \"look\"\n",
    "            return command\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            pass  # Try stopping the game prematurely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a91dec8e-5c85-4217-821b-8da0d8fbd9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-game.z8..........  \tavg. steps: 100.0; avg. score:  4.2 / 10.\n"
     ]
    }
   ],
   "source": [
    "# make the random agent play\n",
    "nb_episodes = 10\n",
    "max_steps = 100\n",
    "\n",
    "play(RandomAgent(), \"./games/test-game.z8\", max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "044be42e-557c-49d4-bbc3-17a911841690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-game.z8..........  \tavg. steps: 100.0; avg. score:  2.0 / 10.\n",
      "Model took 2.866 min to play 10 games\n"
     ]
    }
   ],
   "source": [
    "# make Qwen play\n",
    "\n",
    "start = time.time()\n",
    "play(HFAgent(model, tokenizer), \"./games/test-game.z8\", max_steps=max_steps, nb_episodes=nb_episodes)\n",
    "end = time.time()\n",
    "print(f\"Model took {((end - start)/60):.3f} min to play {nb_episodes} games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6218fd2a-e5b0-4d05-b17c-4ff1e92ca057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-game.z8..........  \tavg. steps:  69.0; avg. score:  8.8 / 10.\n",
      "Model took 15.795 min to play 10 games\n"
     ]
    }
   ],
   "source": [
    "# version with Qwen 4B\n",
    "\n",
    "start = time.time()\n",
    "play(HFAgent(model, tokenizer), \"./games/test-game.z8\", max_steps=max_steps, nb_episodes=nb_episodes)\n",
    "end = time.time()\n",
    "print(f\"Model took {((end - start)/60):.3f} min to play {nb_episodes} games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c4f635c-faf8-4b34-b8ba-66d6031ef48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-game.z8..........  \tavg. steps: 100.0; avg. score:  8.1 / 10.\n",
      "Model took 41.075 min to play 10 games\n"
     ]
    }
   ],
   "source": [
    "# version with Qwen 4B-FP8\n",
    "\n",
    "start = time.time()\n",
    "play(HFAgent(model, tokenizer), \"./games/test-game.z8\", max_steps=max_steps, nb_episodes=nb_episodes)\n",
    "end = time.time()\n",
    "print(f\"Model took {((end - start)/60):.3f} min to play {nb_episodes} games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bfa9f-6023-4c75-8664-77277e9070e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
