{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29df1566",
   "metadata": {},
   "source": [
    "# Test 2 - Model without reasoning _vs_ model with reasoning _vs_ models with reasoning every $x$ turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccc2ada6-c297-436f-a4ea-c7ecda7b442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar czf Testing.tar *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55cbbde-d821-4fa0-ac9e-18fde6c19c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep  9 22:29:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 42%   53C    P8             32W /  450W |      15MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7f88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import textworld\n",
    "import textworld.gym\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "from typing import Mapping, Any\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f499211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "torch.set_default_device('cuda')\n",
    "torch.cuda.device(\"cuda\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04dfd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-4B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "219eb39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d02e48a31c40c28e1c1103187e3854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e21c4a-6808-47f3-ab22-ec15e295f13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b294b",
   "metadata": {},
   "source": [
    "## Play function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb66068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(agent, path, max_steps=100, n_episodes=10, verbose=True):\n",
    "    torch.manual_seed(46)  # For reproducibility when using action sampling.\n",
    "\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
    "\n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
    "\n",
    "    env_id = textworld.gym.register_games(gamefiles,\n",
    "                                          request_infos=infos_to_request,\n",
    "                                          max_episode_steps=max_steps)\n",
    "    env = textworld.gym.make(env_id)  # Create a Gym environment to play the text game.\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path), end=\"\")\n",
    "        else:\n",
    "            print(os.path.basename(path), end=\"\")\n",
    "\n",
    "    # Collect some statistics\n",
    "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
    "    moves_scores_times_list = []\n",
    "    \n",
    "    for no_episode in range(n_episodes):\n",
    "        episode_start = time.process_time()\n",
    "        obs, infos = env.reset()  # Start new episode.\n",
    "\n",
    "        score = 0\n",
    "        done = False\n",
    "        nb_moves = 0\n",
    "        moves_scores_times = [(0, 0, 0)] # starting point\n",
    "        \n",
    "        while not done:\n",
    "            command = agent.act(obs, score, done, infos)\n",
    "            timestamp = time.process_time()\n",
    "            obs, score, done, infos = env.step(command)\n",
    "            nb_moves += 1\n",
    "            moves_scores_times.append((nb_moves, score, timestamp - episode_start))\n",
    "\n",
    "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
    "        moves_scores_times_list.append(moves_scores_times)\n",
    "\n",
    "        if verbose:\n",
    "            print(\".\", end=\"\")\n",
    "        avg_moves.append(nb_moves)\n",
    "        avg_scores.append(score)\n",
    "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
    "\n",
    "    env.close()\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
    "            if len(avg_moves) > 1:\n",
    "                print(f\"Detailed steps: {avg_moves}\\t Detailed normalized scores: {avg_norm_scores}\")\n",
    "        else:\n",
    "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))\n",
    "            if len(avg_moves) > 1:\n",
    "                print(f\"Detailed steps: {avg_moves}\\t Detailed scores: {avg_scores}\")\n",
    "        return moves_scores_times_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9722b791",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19ea048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent(textworld.gym.Agent):\n",
    "    \"\"\"LLM from HuggingFace that acts as an agent.\"\"\"\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    context = \"\"\n",
    "\n",
    "    token_think = \"/think\"\n",
    "    token_nothink = \"/no_think\"\n",
    "    id_token_open_think = None # <think> . TODO find it\n",
    "    id_token_close_think = 151668 # </think>\n",
    "    token_system = \"<|im_start|>system\\n\"\n",
    "    token_endofturn = \"<|im_end|>\\n\"\n",
    "    token_user = \"<|im_start|>user\\n\"\n",
    "    token_assistant = \"<|im_start|>assistant\\n\"\n",
    "    system_prompt = \"\"\"\n",
    "You are an assistant playing a textual game.\n",
    "The user gives you information on the environment and you reply exclusively in the form \\\"verb noun\\\", like \\\"open box\\\" or \\\"take key\\\".\n",
    "/no_think\n",
    "\"\"\"\n",
    "    first_move = False\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.initialize_context()\n",
    "\n",
    "    def initialize_context(self):\n",
    "        self.context = self.token_system + self.system_prompt + self.token_endofturn\n",
    "        self.first_move = True\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands=True)\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "\n",
    "        if done:\n",
    "            self.initialize_context() # resets context\n",
    "            return \":)\"\n",
    "            \n",
    "        if self.first_move:\n",
    "            self.first_move = False\n",
    "            return \"help\"\n",
    "        \n",
    "        try:\n",
    "            self.context += self.token_user + obs + self.token_endofturn\n",
    "            self.context += self.token_assistant # induces model to generate answer\n",
    "            \n",
    "            input_ids = self.tokenizer.encode(\n",
    "                self.context,\n",
    "                return_tensors = \"pt\")\n",
    "            \n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids.to(\"cuda\"),\n",
    "                max_new_tokens = 100,\n",
    "                eos_token_id = self.tokenizer.eos_token_id\n",
    "                )\n",
    "            output_ids = generated_ids[0][len(input_ids[0]):].tolist() \n",
    "            \n",
    "            # parsing thinking content\n",
    "            try:\n",
    "                # index finding </think>\n",
    "                index = len(output_ids) - output_ids[::-1].index(self.id_token_close_think)\n",
    "            except ValueError:\n",
    "                index = 0\n",
    "            response = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "            \n",
    "            self.context += response + self.token_endofturn\n",
    "\n",
    "            if len(response.split()) <= 10:\n",
    "                command = response\n",
    "            else: # more than 10 words, output is surely wrong\n",
    "                command = \"look\"\n",
    "            return command\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            pass  # Try stopping the game prematurely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541dfd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgentSelfEvaluate(LLMAgent):\n",
    "    \"\"\"LLM from HuggingFace that acts as an agent. It self-evaluates its status and moves.\"\"\"\n",
    "\n",
    "    selfeval_turn_counter = 0\n",
    "    selfeval_turns = 5\n",
    "    shorter_reasoning = False\n",
    "    handheld = True\n",
    "    verbose = False\n",
    "    reads_own_reasoning = False\n",
    "\n",
    "    def __init__(self, model, tokenizer, selfeval_turns = 5, handheld = True, verbose = False, reads_own_reasoning = False):\n",
    "        \"\"\"Initialization function.\n",
    "        selfeval_turns: how many turns should pass between a self-evaluation and the next one.\n",
    "        handheld: if this is set to True there are a few simple changes in the function that make it easier for the LLM to understand and correct its course\n",
    "        reads_own_reasoning: if the model's reasoning during the self-evaluation turns should be included in the context too, or only its final action\n",
    "        \"\"\"\n",
    "        super().__init__(model, tokenizer)\n",
    "        if selfeval_turns == 0:\n",
    "            selfeval_turns = -1 # actual default value for deactivating self-evaluation\n",
    "        elif 1 <= selfeval_turns <= 4: # model will think a lot so we need to take care of the context. TODO\n",
    "            self.shorter_reasoning = True\n",
    "        self.selfeval_turns = selfeval_turns\n",
    "        self.handheld = handheld\n",
    "        self.verbose = verbose\n",
    "        self.reads_own_reasoning = reads_own_reasoning\n",
    "\n",
    "    def initialize_context(self):\n",
    "        super().initialize_context()\n",
    "        self.selfeval_turn_counter = 0\n",
    "\n",
    "    def generate_response(self, think=False):\n",
    "        if think:\n",
    "            max_new_tokens = 20000 # allow reasoning models to be talkative\n",
    "        else:\n",
    "            max_new_tokens = 20 # reduce generation almost to a minimum\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(\n",
    "                self.context,\n",
    "                return_tensors = \"pt\")\n",
    "        try:\n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids.to(\"cuda\"),\n",
    "                max_new_tokens = max_new_tokens,\n",
    "                eos_token_id = self.tokenizer.eos_token_id\n",
    "                )\n",
    "            output_ids = generated_ids[0][len(input_ids[0]):].tolist()\n",
    "        except:\n",
    "            return \"help\" # model is in distress :)\n",
    "\n",
    "        if len(output_ids) >= max_new_tokens: # reached cap -- let's help the model a bit\n",
    "            substitute_command = random.choice([\"help\", \"look\"])\n",
    "            if think:\n",
    "                return (\"\", substitute_command)\n",
    "            else:\n",
    "                return substitute_command\n",
    "        \n",
    "        try:\n",
    "            # index finding </think>\n",
    "            index = len(output_ids) - output_ids[::-1].index(self.id_token_close_think)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "        if think:\n",
    "            thinking_response = tokenizer.decode(output_ids[:index], skip_special_tokens=True) \\\n",
    "            .replace(\"/think\", \"\") \\\n",
    "            .replace(\"/no_think\", \"\") \\\n",
    "            .strip(\"\\n\")\n",
    "            response = tokenizer.decode(output_ids[(index+1):], skip_special_tokens=True) \\\n",
    "            .replace(\"<think>\", \"\") \\\n",
    "            .replace(\"</think>\", \"\") \\\n",
    "            .replace(\"/think\", \"\") \\\n",
    "            .replace(\"/no_think\", \"\") \\\n",
    "            .strip(\"\\n\")\n",
    "            return (thinking_response, response)\n",
    "        else:\n",
    "            response = tokenizer.decode(output_ids[index:], skip_special_tokens=True) \\\n",
    "            .replace(\"<think>\", \"\") \\\n",
    "            .replace(\"</think>\", \"\") \\\n",
    "            .replace(\"/think\", \"\") \\\n",
    "            .replace(\"/no_think\", \"\") \\\n",
    "            .strip(\"\\n\")\n",
    "            return response\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        if done:\n",
    "            self.initialize_context() # resets context\n",
    "            return \":)\"\n",
    "        \n",
    "        if self.selfeval_turn_counter == self.selfeval_turns: # time for self-evaluation\n",
    "            self.selfeval_turn_counter = 0 # reset counter\n",
    "            return self.self_evaluation(obs)\n",
    "            \n",
    "        try:\n",
    "            self.context += self.token_user + obs + self.token_endofturn\n",
    "            self.context += self.token_assistant # induces model to generate answer\n",
    "\n",
    "            if self.first_move and self.handheld:\n",
    "                self.first_move = False\n",
    "                command = \"help\"\n",
    "            else:\n",
    "                response = self.generate_response()\n",
    "                if len(response.split()) <= 10 or not self.handheld:\n",
    "                    command = response\n",
    "                else: # more than 10 words, output is surely wrong\n",
    "                    command = \"look\"\n",
    "            \n",
    "            self.context += command + self.token_endofturn\n",
    "\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"GAME ++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "                print(obs)\n",
    "                print(\"AGENT -------------------------------------------------\")\n",
    "                print(command)\n",
    "\n",
    "            self.selfeval_turn_counter += 1\n",
    "            return command\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            pass  # Try stopping the game prematurely.\n",
    "\n",
    "    def self_evaluation(self, obs) -> str :\n",
    "        if self.shorter_reasoning:\n",
    "            self_evaluation_prompt = \"\"\"\n",
    "Do you think you're making the right actions in the game? Think about it briefly, and then say your next action in the same way as before.\n",
    "\"\"\"\n",
    "        else:\n",
    "            self_evaluation_prompt = \"\"\"\n",
    "Do you think you're making the right actions in the game? Do you think you're close to reaching the original goal? Think about it. \n",
    "\"\"\"\n",
    "        self.context += self.token_user + obs + self_evaluation_prompt + self.token_think + self.token_endofturn \n",
    "        self.context += self.token_assistant # induce thinking\n",
    "        \n",
    "        (thinking_response, response) = self.generate_response(think=True)\n",
    "        if self.verbose:\n",
    "            print(\"GAME ++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "            print(obs + self_evaluation_prompt + self.token_think)\n",
    "            print(\"SELF-EVALUATION: +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\")\n",
    "            print(thinking_response + response + self.token_nothink)\n",
    "\n",
    "        if self.reads_own_reasoning:\n",
    "            self.context += thinking_response + response + self.token_nothink + self.token_endofturn\n",
    "        else:\n",
    "            self.context += response + self.token_nothink + self.token_endofturn\n",
    "\n",
    "        if len(response.split()) <= 10 or not self.handheld:\n",
    "            command = response\n",
    "        else: # more than 10 words, output is surely wrong\n",
    "            command = \"look\"\n",
    "        self.selfeval_turn_counter += 1\n",
    "        return command\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa9f3f",
   "metadata": {},
   "source": [
    "## Game generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "497c8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100\n",
    "n_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f48939f-f0c6-471f-aeef-14e23bc357fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 to 10\n",
    "seeds = range(1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f880c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 to 9\n",
    "seeds = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ee7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polietilene\n",
    "seeds = [6, 10, 20, 45, 46, 89, 79010123, 1179382318]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df11440-0b59-4f09-9eb8-abcfb64dc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tw-make tw-simple --rewards dense --goal detailed --seed 0 --test --silent -f --output games/test-seed0.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 1 --test --silent -f --output games/test-seed1.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 2 --test --silent -f --output games/test-seed2.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 3 --test --silent -f --output games/test-seed3.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 4 --test --silent -f --output games/test-seed4.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 5 --test --silent -f --output games/test-seed5.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 6 --test --silent -f --output games/test-seed6.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 7 --test --silent -f --output games/test-seed7.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 8 --test --silent -f --output games/test-seed8.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 9 --test --silent -f --output games/test-seed9.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 10 --test --silent -f --output games/test-seed10.z8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2e40c",
   "metadata": {},
   "source": [
    "## Game running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5aaeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_seeds(mode, verbose = False):\n",
    "    if mode == \"nothink\" or mode == -1:\n",
    "        selfeval_turns = -1\n",
    "        mode_name = \"nothink\"\n",
    "    elif mode == \"fullthink\" or mode == 1:\n",
    "        selfeval_turns = 1\n",
    "        mode_name = \"fullthink\"\n",
    "    elif type(mode) is not type(1):\n",
    "        return \"Error! Mode not recognized.\"\n",
    "    else:\n",
    "        selfeval_turns = mode\n",
    "        mode_name = f\"{mode}-think\"\n",
    "        \n",
    "    for seed in seeds:\n",
    "        results = play(LLMAgentSelfEvaluate(model, tokenizer, selfeval_turns = selfeval_turns, handheld=False, verbose=verbose),\n",
    "                f\"./games/test-seed{seed}.z8\", max_steps=max_steps, n_episodes=n_episodes)\n",
    "        with open(f'./Testing 2/{mode_name}_seed{seed}.pickle', 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "            print(\"Data pickled.\")\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80bc29e1-fe10-4eeb-8d0d-e4ec1806269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-seed1.z8..........  \tavg. steps:  89.6; avg. score:  7.2 / 8.\n",
      "Detailed steps: [100, 100, 100, 100, 100, 47, 100, 100, 49, 100]\t Detailed scores: [7, 7, 7, 7, 7, 8, 7, 7, 8, 7]\n",
      "Data pickled.\n",
      "test-seed2.z8..........  \tavg. steps: 100.0; avg. score:  8.0 / 10.\n",
      "Detailed steps: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\t Detailed scores: [9, 9, 9, 9, 9, 4, 9, 9, 9, 4]\n",
      "Data pickled.\n",
      "test-seed3.z8..........  \tavg. steps:   8.0; avg. score:  7.0 / 7.\n",
      "Detailed steps: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed4.z8..........  \tavg. steps: 100.0; avg. score:  5.5 / 10.\n",
      "Detailed steps: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\t Detailed scores: [2, 2, 2, 9, 2, 9, 9, 9, 2, 9]\n",
      "Data pickled.\n",
      "test-seed5.z8..........  \tavg. steps: 100.0; avg. score:  4.0 / 7.\n",
      "Detailed steps: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\t Detailed scores: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Data pickled.\n",
      "test-seed6.z8..........  \tavg. steps:  91.2; avg. score:  9.1 / 10.\n",
      "Detailed steps: [100, 100, 100, 100, 12, 100, 100, 100, 100, 100]\t Detailed scores: [9, 9, 9, 9, 10, 9, 9, 9, 9, 9]\n",
      "Data pickled.\n",
      "test-seed7.z8..........  \tavg. steps:   9.3; avg. score:  7.0 / 7.\n",
      "Detailed steps: [9, 9, 9, 9, 9, 10, 9, 9, 10, 10]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed8.z8..........  \tavg. steps:  75.4; avg. score:  9.3 / 10.\n",
      "Detailed steps: [100, 100, 100, 100, 100, 100, 100, 18, 18, 18]\t Detailed scores: [9, 9, 9, 9, 9, 9, 9, 10, 10, 10]\n",
      "Data pickled.\n",
      "test-seed9.z8..........  \tavg. steps:   9.8; avg. score:  7.0 / 7.\n",
      "Detailed steps: [10, 10, 10, 10, 9, 10, 10, 10, 9, 10]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed10.z8..........  \tavg. steps:   8.7; avg. score:  7.0 / 7.\n",
      "Detailed steps: [10, 9, 8, 8, 10, 8, 8, 10, 8, 8]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n"
     ]
    }
   ],
   "source": [
    "play_seeds(\"nothink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66666703",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_seeds(\"fullthink\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be0c212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-seed1.z8..........  \tavg. steps:  21.8; avg. score:  7.9 / 8.\n",
      "Detailed steps: [11, 11, 11, 11, 100, 30, 11, 11, 11, 11]\t Detailed scores: [8, 8, 8, 8, 7, 8, 8, 8, 8, 8]\n",
      "Data pickled.\n",
      "test-seed2.z8..........  \tavg. steps:  21.3; avg. score: 10.0 / 10.\n",
      "Detailed steps: [16, 18, 16, 37, 22, 21, 21, 18, 16, 28]\t Detailed scores: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "Data pickled.\n",
      "test-seed3.z8..........  \tavg. steps:   8.0; avg. score:  7.0 / 7.\n",
      "Detailed steps: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed4.z8..........  \tavg. steps:  74.8; avg. score:  4.4 / 10.\n",
      "Detailed steps: [100, 16, 100, 100, 16, 16, 100, 100, 100, 100]\t Detailed scores: [2, 10, 2, 2, 10, 10, 2, 2, 2, 2]\n",
      "Data pickled.\n",
      "test-seed5.z8..........  \tavg. steps:  20.2; avg. score:  6.7 / 7.\n",
      "Detailed steps: [10, 10, 10, 11, 10, 10, 10, 100, 21, 10]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 4, 7, 7]\n",
      "Data pickled.\n",
      "test-seed6.z8..........  \tavg. steps:  12.4; avg. score: 10.0 / 10.\n",
      "Detailed steps: [16, 12, 12, 12, 12, 12, 12, 12, 12, 12]\t Detailed scores: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "Data pickled.\n",
      "test-seed7.z8..........  \tavg. steps:  10.3; avg. score:  7.0 / 7.\n",
      "Detailed steps: [11, 10, 10, 10, 11, 11, 10, 10, 10, 10]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed8.z8..........  \tavg. steps:  18.7; avg. score: 10.0 / 10.\n",
      "Detailed steps: [21, 18, 19, 18, 18, 17, 21, 19, 18, 18]\t Detailed scores: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "Data pickled.\n",
      "test-seed9.z8..........  \tavg. steps:  10.0; avg. score:  7.0 / 7.\n",
      "Detailed steps: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed10.z8..........  \tavg. steps:  11.3; avg. score:  7.0 / 7.\n",
      "Detailed steps: [10, 8, 8, 16, 13, 15, 8, 13, 8, 14]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n"
     ]
    }
   ],
   "source": [
    "play_seeds(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f7a5778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-seed1.z8..........  \tavg. steps:  11.0; avg. score:  8.0 / 8.\n",
      "Detailed steps: [11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\t Detailed scores: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "Data pickled.\n",
      "test-seed2.z8..........  \tavg. steps:  37.5; avg. score: 10.0 / 10.\n",
      "Detailed steps: [23, 41, 31, 42, 31, 31, 22, 72, 41, 41]\t Detailed scores: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "Data pickled.\n",
      "test-seed3.z8..........  \tavg. steps:   8.0; avg. score:  7.0 / 7.\n",
      "Detailed steps: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed4.z8..........  \tavg. steps:  80.9; avg. score:  6.7 / 10.\n",
      "Detailed steps: [100, 56, 71, 100, 47, 100, 100, 35, 100, 100]\t Detailed scores: [3, 10, 10, 2, 10, 2, 9, 10, 2, 9]\n",
      "Data pickled.\n",
      "test-seed5.z8..........  \tavg. steps:  30.7; avg. score:  7.0 / 7.\n",
      "Detailed steps: [26, 37, 27, 21, 66, 16, 21, 16, 61, 16]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed6.z8..........  \tavg. steps:  12.0; avg. score: 10.0 / 10.\n",
      "Detailed steps: [12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\t Detailed scores: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "Data pickled.\n",
      "test-seed7.z8..........  \tavg. steps:   9.3; avg. score:  7.0 / 7.\n",
      "Detailed steps: [9, 9, 9, 9, 9, 10, 9, 9, 10, 10]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed8.z8..........  \tavg. steps:  37.0; avg. score: 10.0 / 10.\n",
      "Detailed steps: [41, 22, 23, 41, 22, 51, 22, 23, 32, 93]\t Detailed scores: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "Data pickled.\n",
      "test-seed9.z8..........  \tavg. steps:   9.8; avg. score:  7.0 / 7.\n",
      "Detailed steps: [10, 10, 10, 10, 9, 10, 10, 10, 9, 10]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n",
      "test-seed10.z8..........  \tavg. steps:   8.7; avg. score:  7.0 / 7.\n",
      "Detailed steps: [10, 9, 8, 8, 10, 8, 8, 10, 8, 8]\t Detailed scores: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "Data pickled.\n"
     ]
    }
   ],
   "source": [
    "play_seeds(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098ee46-5dff-41fb-adb4-1851b318ea84",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38427b-f8b3-4c1e-90cd-915984b73adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nothink = []\n",
    "results_fullthink = []\n",
    "results_5think = []\n",
    "results_10think = []\n",
    "\n",
    "for seed in seeds:\n",
    "    with open(f'nothink_seed{seed}.pickle', 'rb') as f:\n",
    "        results_nothink.append(pickle.load(f))\n",
    "        f.close()\n",
    "    with open(f'fullthink_seed{seed}.pickle', 'rb') as f:\n",
    "        results_fullthink.append(pickle.load(f))\n",
    "        f.close()\n",
    "    with open(f'5-think_seed{seed}.pickle', 'rb') as f:\n",
    "        results_5think.append(pickle.load(f))\n",
    "        f.close()\n",
    "    with open(f'10-think_seed{seed}.pickle', 'rb') as f:\n",
    "        results_10think.append(pickle.load(f))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a7b5c-36ed-4836-a0e2-a8f85fa30d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scores = [8, 10, 7, 10, 7, 10, 7, 10, 7, 7]\n",
    "avg_scores_nothink = []\n",
    "std_nothink = []\n",
    "avg_scores_fullthink = []\n",
    "std_fullthink = []\n",
    "avg_scores_5think = []\n",
    "std_5think = []\n",
    "avg_scores_10think = []\n",
    "std_10think = []\n",
    "results_scores_std = [(results_nothink, avg_scores_nothink, std_nothink),\n",
    "                (results_fullthink, avg_scores_fullthink, std_fullthink),\n",
    "                (results_5think, avg_scores_5think, std_5think),\n",
    "                (results_10think, avg_scores_10think, std_10think)]\n",
    "\n",
    "for model in results_scores_std:\n",
    "    results, avg_scores, std = model\n",
    "    for seed in seeds:\n",
    "        scores_of_final_steps = []\n",
    "        runs = results[seed-1]\n",
    "        for run in runs:\n",
    "            scores_of_final_steps.append(run[-1][1]) # 2nd parameter (aka score) of last step\n",
    "        avg_scores.append(np.mean(scores_of_final_steps) / max_scores[seed-1])\n",
    "        std.append(np.std(scores_of_final_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911368f-00b5-4fe2-885b-4745c57fd5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.13, 0.19999999999999998, 0.2, 0.5714285714285714, 0.02, 0.15714285714285717, 0.0, 0.14285714285714285, 0.2857142857142857]\n",
      "[0.9, 0.8, 1.0, 0.55, 0.5714285714285714, 0.9099999999999999, 1.0, 0.93, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(avg_scores_nothink)\n",
    "print(std_nothink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c3bd4-4cd6-4748-ba2d-504f7e93b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_times_nothink = []\n",
    "std_times_nothink = []\n",
    "avg_times_fullthink = []\n",
    "std_times_fullthink = []\n",
    "avg_times_5think = []\n",
    "std_times_5think = []\n",
    "avg_times_10think = []\n",
    "std_times_10think = []\n",
    "results_times_std = [(results_nothink, avg_times_nothink, std_times_nothink),\n",
    "                (results_fullthink, avg_times_fullthink, std_times_fullthink),\n",
    "                (results_5think, avg_times_5think, std_times_5think),\n",
    "                (results_10think, avg_times_10think, std_times_10think)]\n",
    "\n",
    "for model in results_times_std:\n",
    "    results, avg_times, std_times = model\n",
    "    for seed in seeds:\n",
    "        times_of_final_steps = []\n",
    "        runs = results[seed-1]\n",
    "        for run in runs:\n",
    "            times_of_final_steps.append(run[-1][2]) # 3rd parameter (aka time) of last step\n",
    "        avg_times.append(np.mean(times_of_final_steps))\n",
    "        std_times.append(np.std(scores_of_final_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce4db2-fb63-47d2-b2bc-3add4567a7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.6312863671, 16.674585863100017, 15.095549504599996, 17.00770652959998, 17.497499273999995, 19.447194486599994, 18.014292129699992, 19.38664712999996, 14.100355692300013, 17.882568143800064]\n",
      "[28.436508070499997, 31.7440423399, 1.2397813183000153, 49.16057651520005, 26.665991287099995, 41.816298050900016, 1.5289465187999896, 33.51782474239999, 1.659197742799961, 1.3803591473999859]\n"
     ]
    }
   ],
   "source": [
    "print(avg_times_nothink)\n",
    "print(std_times_nothink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d31c2d-b310-4c22-89e1-550cd0b924c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(seeds) # converting into a list just to be sure, maybe an iterator doesn't work here\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, avg_scores_nothink, label=\"no reasoning\", marker=\".\", linestyle=\"-\", color=\"red\")\n",
    "ax.plot(x, avg_scores_10think, label=\"10-turn reasoning\", marker=\".\", linestyle=\"-\", color=\"orange\")\n",
    "ax.plot(x, avg_scores_5think, label=\"5-turn reasoning\", marker=\".\", linestyle=\"-\", color=\"gold\")\n",
    "ax.plot(x, avg_scores_fullthink, label=\"full reasoning\", marker=\".\", linestyle=\"-\", color=\"greenyellow\")\n",
    "# ax.fill_between(x, avg_scores_1_7B, avg_scores_4B, alpha=0.1, color=\"purple\")\n",
    "\n",
    "ax.set_title(\"Average final score on 10 different games\")\n",
    "ax.set_xlabel('game seed')\n",
    "ax.set_ylabel('average final score, normalized')\n",
    "\n",
    "gridlines = np.arange(x[0], x[-1]+1, 1)\n",
    "ax.set_xticks(gridlines)\n",
    "ax.grid(axis=\"x\", alpha=0.2)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7ae14-c9fa-4294-9994-bf3bba07582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, avg_scores_nothink, label=\"no reasoning\", marker=\".\", linestyle=\"-\", color=\"red\")\n",
    "ax.plot(x, avg_scores_10think, label=\"10-turn reasoning\", marker=\".\", linestyle=\"-\", color=\"orange\")\n",
    "ax.plot(x, avg_scores_5think, label=\"5-turn reasoning\", marker=\".\", linestyle=\"-\", color=\"gold\")\n",
    "ax.plot(x, avg_scores_fullthink, label=\"full reasoning\", marker=\".\", linestyle=\"-\", color=\"greenyellow\")\n",
    "\n",
    "ax.set_title(\"Average time to complete or run out of moves\")\n",
    "ax.set_xlabel('game seed')\n",
    "ax.set_ylabel('average final time [s]')\n",
    "\n",
    "gridlines = np.arange(x[0], x[-1]+1, 1)\n",
    "ax.set_xticks(gridlines)\n",
    "ax.grid(axis=\"x\", alpha=0.2)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46093a",
   "metadata": {},
   "source": [
    "# Bundles of tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to use a seed with 10 total score, for clarity\n",
    "bundle_seed = 11\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 11 --test --silent -f --output games/test-seed11.z8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = play(LLMAgentSelfEvaluate(model, tokenizer, selfeval_turns = -1, handheld=False, verbose=False),\n",
    "        f\"./games/test-seed{bundle_seed}.z8\", max_steps=max_steps, n_episodes=n_episodes)\n",
    "\n",
    "with open(f'./Testing 2/nothink_seed{seed}.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "    print(\"Data pickled.\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = play(LLMAgentSelfEvaluate(model, tokenizer, selfeval_turns = 10, handheld=False, verbose=False),\n",
    "        f\"./games/test-seed{bundle_seed}.z8\", max_steps=max_steps, n_episodes=n_episodes)\n",
    "\n",
    "with open(f'./Testing 2/10think_seed{seed}.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "    print(\"Data pickled.\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_score_index = 1 # (step number, cumulative score, timestamp from beginning of episode)\n",
    "\n",
    "bundle_nothink = []\n",
    "bundle_10think = []\n",
    "cum_scores_nothink = []\n",
    "cum_scores_10think = []\n",
    "\n",
    "bundles_cum_scores = [(bundle_nothink, cum_scores_nothink), (bundle_10think, cum_scores_10think)]\n",
    "\n",
    "with open(f'nothink_seed{bundle_seed}.pickle', 'rb') as f:\n",
    "        bundle_nothink.append(pickle.load(f))\n",
    "        f.close()\n",
    "with open(f'10think_seed{bundle_seed}.pickle', 'rb') as f:\n",
    "        bundle_10think.append(pickle.load(f))\n",
    "        f.close()\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    for bundle, cum_scores in bundles_cum_scores:\n",
    "        single_run = []\n",
    "        for step in bundle:\n",
    "            single_run.append(step[cum_score_index])\n",
    "        cum_scores.append(single_run)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99474be5-e1fc-40cf-82d8-89b3de383976",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "moves = np.arange(0, max_steps+1, 1)\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    if i == 0: # set labels just once\n",
    "        ax.plot(moves, cum_scores_nothink[i], marker=\".\", linestyle=\"-\", color=\"red\", label=\"no reasoning\")\n",
    "        ax.plot(moves, cum_scores_10think[i], marker=\".\", linestyle=\"-\", color=\"orange\", label = \"10-turn reasoning\")\n",
    "    else:\n",
    "        ax.plot(moves, cum_scores_nothink[i], marker=\".\", linestyle=\"-\", color=\"red\")\n",
    "        ax.plot(moves, cum_scores_10think[i], marker=\".\", linestyle=\"-\", color=\"orange\")\n",
    "\n",
    "\n",
    "ax.set_title(\"Comparison over 10x2 runs, no reasoning vs 10-turn reasoning\")\n",
    "ax.set_xlabel('moves')\n",
    "ax.set_ylabel('cumulative score')\n",
    "\n",
    "major_ticks = np.arange(11, max_steps+1, 10)\n",
    "minor_ticks = np.arange(1, max_steps+1, 5)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.grid(axis=\"x\", which='minor', alpha=0.2)\n",
    "ax.grid(axis=\"x\", which='major', alpha=0.5)\n",
    "ax.minorticks_on() # is this still needed since we set some minor ticks? idk\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
