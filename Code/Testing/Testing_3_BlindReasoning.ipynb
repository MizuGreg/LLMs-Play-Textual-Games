{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29df1566",
   "metadata": {},
   "source": [
    "# Test 3 (+ test 5) - Blind vs non-blind, plotted over x-think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccc2ada6-c297-436f-a4ea-c7ecda7b442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar czf Testing.tar *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55cbbde-d821-4fa0-ac9e-18fde6c19c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textworld\n",
    "import textworld.gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "from typing import Mapping, Any\n",
    "\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f499211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_device('cuda')\n",
    "torch.cuda.device(\"cuda\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63dbb998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imptools\n",
      "  Downloading imptools-1.3.0.tar.gz (4.1 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: imptools\n",
      "  Building wheel for imptools (pyproject.toml): started\n",
      "  Building wheel for imptools (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for imptools: filename=imptools-1.3.0-py3-none-any.whl size=5422 sha256=885c1cded71b8c3811109516c10ea35a98a92e8c8e1a186d8bfcb7456cda8ea8\n",
      "  Stored in directory: c:\\users\\greg\\appdata\\local\\pip\\cache\\wheels\\a7\\1b\\f4\\82873dac15e1260329d662bd50ea1bd0f693b573b805461d37\n",
      "Successfully built imptools\n",
      "Installing collected packages: imptools\n",
      "Successfully installed imptools-1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install imptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529d22da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textworld'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m\"\u001b[39m\u001b[33m../Self_evaluation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mself_evaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m play, LLMAgentSelfEvaluate\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Greg\\Desktop\\Tesi\\LLMs-Play-Textual-Games\\Code\\Testing\\../Self_evaluation\\self_evaluation.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextworld\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextworld\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'textworld'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Self_evaluation\")\n",
    "\n",
    "from self_evaluation import play, LLMAgentSelfEvaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa9f3f",
   "metadata": {},
   "source": [
    "## Game generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48939f-f0c6-471f-aeef-14e23bc357fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 to 20 included\n",
    "seeds = range(11,21)\n",
    "max_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df11440-0b59-4f09-9eb8-abcfb64dc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tw-make tw-simple --rewards dense --goal detailed --seed 11 --test --silent -f --output games/test-seed11.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 12 --test --silent -f --output games/test-seed12.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 13 --test --silent -f --output games/test-seed13.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 14 --test --silent -f --output games/test-seed14.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 15 --test --silent -f --output games/test-seed15.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 16 --test --silent -f --output games/test-seed16.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 17 --test --silent -f --output games/test-seed17.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 18 --test --silent -f --output games/test-seed18.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 19 --test --silent -f --output games/test-seed19.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 20 --test --silent -f --output games/test-seed20.z8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da22e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memes\n",
    "seeds = [6, 10, 11, 20, 45, 46, 89, 1863, 79010123, 1179382318]\n",
    "max_scores = [10, 7, 7, 7, 10, 7, 10, 7, 10, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tw-make tw-simple --rewards dense --goal detailed --seed 6 --test --silent -f --output games/test-seed6.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 10 --test --silent -f --output games/test-seed10.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 11 --test --silent -f --output games/test-seed11.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 20 --test --silent -f --output games/test-seed20.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 45 --test --silent -f --output games/test-seed45.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 46 --test --silent -f --output games/test-seed46.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 89 --test --silent -f --output games/test-seed89.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 1863 --test --silent -f --output games/test-seed1863.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 79010123 --test --silent -f --output games/test-seed79010123.z8\n",
    "!tw-make tw-simple --rewards dense --goal detailed --seed 1179382318 --test --silent -f --output games/test-seed1179382318.z8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2e40c",
   "metadata": {},
   "source": [
    "## Game running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100\n",
    "n_episodes = 1\n",
    "n_think_indices = range(0,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_think_indices:\n",
    "    for read_bool in (True, False):\n",
    "        for seed in seeds:\n",
    "            results = play(LLMAgentSelfEvaluate(selfeval_turns=n, reads_own_reasoning=read_bool, verbose=True),\n",
    "                           f\"games/test-seed{seed}\", max_steps =max_steps,  n_episodes=n_episodes)\n",
    "            with open(f'./Testing 3/{n}think_{\"blind_\" if not read_bool else \"\"}_seed{seed}.pickle', 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "                print(\"Data pickled.\")\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098ee46-5dff-41fb-adb4-1851b318ea84",
   "metadata": {},
   "source": [
    "# Data aggregation for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e07c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_final_scores = np.array()\n",
    "avg_final_scores_ci = np.array()\n",
    "avg_final_scores_blind = np.array()\n",
    "avg_final_scores_blind_ci = np.array()\n",
    "\n",
    "for n in n_think_indices:\n",
    "    for read_bool in (True, False):\n",
    "        avg_final_score = 0\n",
    "        final_scores = []\n",
    "        for seed in seeds:\n",
    "            results = []\n",
    "            with open(f'./Testing 3/{n}think_{\"blind_\" if not read_bool else \"\"}_seed{seed}.pickle', 'rb') as f:\n",
    "                pickle.load(results, f)\n",
    "                print(\"Data pickled.\")\n",
    "                f.close()\n",
    "            final_scores.append(results[0][-1][1] / max_scores[seeds.index(seed)]) # run 1, last step, score normalized\n",
    "\n",
    "        avg_final_score = np.mean(final_scores)\n",
    "        bootstrap_results = bootstrap(data=(final_scores,), \n",
    "                              statistic=np.mean,\n",
    "                              method=\"basic\",\n",
    "                              n_resamples=1000,\n",
    "                              confidence_level=0.9)\n",
    "        if read_bool:\n",
    "            avg_final_scores.append(avg_final_score)\n",
    "            avg_final_scores_ci.append((bootstrap_results.confidence_interval.high, bootstrap_results.confidence_interval.low)) \n",
    "        else:\n",
    "            avg_final_scores_blind.append(avg_final_score)\n",
    "            avg_final_scores_blind_ci.append((bootstrap_results.confidence_interval.high, bootstrap_results.confidence_interval.low)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3085eb",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59179ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_final_scores = np.array()\n",
    "avg_final_scores_ci = np.array()\n",
    "avg_final_scores_blind = np.array()\n",
    "avg_final_scores_blind_ci = np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d31c2d-b310-4c22-89e1-550cd0b924c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = n_think_indices # n-think\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "ax.plot(n, avg_final_scores, label=\"non-ephemeral self-evaluation\", marker=\".\", linestyle=\"-\", color=\"red\")\n",
    "ax.plot(n, avg_final_scores_blind, label=\"ephemeral self-evaluation\", marker=\".\", linestyle=\"-\", color=\"blue\")\n",
    "\n",
    "ax.fill_between(n,\n",
    "                np.clip([i[0] for i in avg_final_scores_ci], 0, 1),\n",
    "                np.clip([i[1] for i in avg_final_scores_ci], 0, 1),\n",
    "                alpha=0.1, color=\"red\")\n",
    "ax.fill_between(n,\n",
    "                np.clip([i[0] for i in avg_final_scores_blind_ci], 0, 1),\n",
    "                np.clip([i[1] for i in avg_final_scores_blind_ci], 0, 1),\n",
    "                alpha=0.1, color=\"blue\")\n",
    "\n",
    "\n",
    "ax.set_title(\"Average final score of an ephemeral/non-ephemeral $n$-think model with $n=0,...,10$\")\n",
    "ax.set_xlabel('n (number non-self-evaluating turns for every self-evaluating turn)')\n",
    "ax.set_ylabel('average final score, normalized')\n",
    "\n",
    "gridlines = np.arange(n[0], n[-1]+1, 1)\n",
    "ax.set_xticks(gridlines)\n",
    "ax.grid(axis=\"x\", alpha=0.2)\n",
    "\n",
    "# ax.set_ylim(0.2, 1.1)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
